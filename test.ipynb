{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-1.5128, -2.2610,  0.3344,  ..., -0.6744, -0.0250,  0.8220],\n",
       "         [-2.1579,  1.5387,  0.2152,  ..., -1.8528,  3.9191, -0.2904],\n",
       "         [ 0.0878,  0.0288, -0.1838,  ...,  0.3139,  0.2148,  0.3052],\n",
       "         ...,\n",
       "         [-1.1054,  2.6932,  1.3499,  ..., -1.2614,  2.3524,  0.6136],\n",
       "         [-0.1871, -1.9390,  0.2427,  ...,  0.0196,  1.4425, -0.8349],\n",
       "         [ 1.3549, -0.1388,  0.2501,  ..., -1.2175,  0.2364,  0.7176]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 3.6017e-02, -3.5049e-01, -4.4201e-01, -2.8682e-01, -5.6795e-01,\n",
       "          2.4877e-01, -4.7875e-01, -1.4376e-01, -4.6937e-01,  1.4841e-01,\n",
       "          6.8588e-02, -1.9145e-01,  3.8480e-02, -3.9165e-02, -2.2866e-01,\n",
       "         -3.1036e-01,  1.4210e-01,  1.4106e-01,  9.5159e-02, -2.2872e-01,\n",
       "          1.4416e-01, -4.0685e-01,  1.4664e-01, -1.5494e-01, -4.9433e-01,\n",
       "         -8.5889e-01,  1.7918e-01, -2.6105e-01, -6.3556e-02,  1.8168e-01,\n",
       "         -4.9847e-01,  3.0679e-01,  4.6911e-01, -3.5475e-02,  5.0747e-01,\n",
       "         -2.3918e-01,  1.1093e-01, -1.5121e-01,  1.5362e-01,  1.1636e-01,\n",
       "          6.4633e-02, -3.5946e-01,  1.7964e-01,  7.0061e-02,  6.5832e-01,\n",
       "          1.0307e-01,  4.7486e-01, -4.4407e-01,  4.6475e-01, -2.5603e-01,\n",
       "          5.4639e-03,  6.5607e-01,  1.4787e-02,  1.0481e-01,  1.3134e+00,\n",
       "          2.9213e-01, -1.0819e-01,  2.3942e-01, -3.0178e-01, -4.2306e-01,\n",
       "          2.5369e-01, -2.0642e-01,  9.7663e-02,  8.3509e-02, -3.9507e-01,\n",
       "         -3.0261e-01,  1.2108e-03,  1.0222e-01,  2.7151e-01, -2.3152e-01,\n",
       "         -1.8672e-01,  1.1255e-01,  1.1748e-02,  3.5656e-01, -2.3398e-01,\n",
       "         -3.7900e-01,  5.5410e-01, -4.6575e-02,  8.0031e+00, -3.2936e-01,\n",
       "         -3.1281e-01, -4.9773e-01, -3.3484e-01, -7.0146e-02,  6.5998e-02,\n",
       "         -2.0500e-01, -4.0121e-02, -7.0238e-01, -5.3058e-01,  1.6623e-01,\n",
       "         -8.7526e-02,  1.6982e-01, -4.0944e-01, -4.9586e-02,  1.3169e+00,\n",
       "         -8.1159e-02, -4.2911e-01, -4.5022e-01, -1.0080e-01,  1.3659e-01,\n",
       "         -9.4163e-02,  4.1847e-01, -1.0119e-01, -3.8974e-01,  1.6861e-01,\n",
       "         -2.0245e-01, -6.2246e-01,  2.6228e-01,  1.0820e-01,  6.8963e-01,\n",
       "         -3.3338e-01,  4.0726e-01, -1.1829e-01,  1.6890e-01, -3.1326e-03,\n",
       "          2.5807e-01,  9.6983e-03,  1.3610e-01, -4.0411e-01,  3.2916e-01,\n",
       "         -9.8216e-02, -2.6031e-01,  3.9084e-01, -1.7741e-02,  8.2868e-01,\n",
       "          3.5638e-01,  1.3315e-01, -2.9415e-01,  1.2510e+00,  4.2406e-01,\n",
       "         -9.4229e-02,  3.5185e-01,  3.5367e-01, -3.3221e-01, -7.1267e-01,\n",
       "         -1.6393e-01, -3.4766e-01,  8.0150e-02,  3.8446e-01,  2.0368e-01,\n",
       "          8.4333e-02,  3.5588e-01, -4.9184e-03,  4.8893e-01,  2.0030e+00,\n",
       "         -7.0017e-01,  1.6537e-01, -3.0750e-01, -1.2466e-01, -3.2410e-01,\n",
       "          4.0012e-01, -2.7865e-01, -1.9315e-01,  3.9778e+00,  3.1379e-01,\n",
       "         -3.6073e-02, -1.5726e-01,  4.8091e-01,  1.8588e-02, -1.5150e-01,\n",
       "         -1.8656e-01,  3.6728e-01,  6.4257e-01, -2.5748e-01,  1.9952e-01,\n",
       "          1.9028e-01, -1.7972e-01,  2.8748e-03, -2.0774e-01,  6.0564e-02,\n",
       "         -1.2797e-01,  3.5441e-01, -3.0040e-01, -8.2838e-02, -1.5758e-01,\n",
       "         -2.7749e-02,  1.2321e-01, -3.3565e-01, -1.7586e-01,  1.8589e-01,\n",
       "         -4.6450e-01, -4.9955e-01,  7.1692e-02, -4.6567e-01, -2.2120e-01,\n",
       "         -2.3646e-02, -3.6857e-01,  6.7738e-01,  1.4390e-01, -7.2590e-02,\n",
       "          3.7936e-01,  8.1345e-02,  4.2240e-01,  5.0051e-01, -5.4146e-01,\n",
       "          2.1101e-01, -6.1715e-01, -1.1160e-01, -3.6426e-01, -7.4611e-03,\n",
       "         -1.2268e-01,  1.6718e-01, -1.6873e-01,  3.9877e-01, -2.1988e-01,\n",
       "          4.3501e-01,  2.4281e-02,  1.4380e+00,  1.9804e-01,  2.5018e-01,\n",
       "         -2.2334e-01,  7.5306e-01,  2.8675e-01, -1.4143e-01,  2.4620e-01,\n",
       "         -1.8822e-01, -4.3588e-01, -5.2313e-01,  1.4898e-01, -2.6456e-02,\n",
       "          5.8068e-01,  6.6992e-03,  3.1424e-01,  2.2717e-01, -1.4572e-01,\n",
       "          2.8105e-01, -1.9674e-01,  3.6837e-01,  6.5800e-02, -5.6088e-01,\n",
       "         -1.7175e-01,  4.1447e-01,  1.0483e-01, -5.5860e-01, -1.6744e-01,\n",
       "          2.5595e-01,  2.3886e-02, -5.1964e-02,  2.4330e-01,  5.5000e-01,\n",
       "         -1.8194e-01, -1.6242e-01, -1.3103e-01,  3.5029e-01,  4.8953e-01,\n",
       "          4.4856e-01,  3.4441e-01,  5.1242e-02, -1.6270e-01,  3.8831e-01,\n",
       "         -4.5184e-01,  3.4622e-01, -3.3040e-01,  6.6391e-03,  3.5370e-01,\n",
       "         -2.5376e-01,  4.4610e-01, -1.1830e+00,  2.2575e-01, -8.3141e-02,\n",
       "          9.0888e-02,  1.8096e-01, -1.3451e-01,  2.9777e-01,  2.4374e-01,\n",
       "          8.4159e-02,  4.4125e-01, -5.8510e-01,  3.0793e-01,  1.9601e-01,\n",
       "         -2.2793e-01,  4.3529e-02,  2.0906e-01, -2.4602e-01,  5.6005e-01,\n",
       "         -6.3090e-01, -1.1706e+00, -2.9278e-01, -4.1054e-02, -3.7422e-02,\n",
       "         -1.0019e+00,  4.7454e-01, -7.8395e-01, -3.3532e-01, -3.5292e-01,\n",
       "          1.8330e-01, -5.9012e-02, -1.5784e-01, -9.3587e-02, -2.1750e-01,\n",
       "          4.2388e-01,  2.3284e-01, -4.1606e-01, -6.6039e-01, -4.6706e-01,\n",
       "         -2.9440e-01,  7.1371e-02, -2.2476e-01, -7.7425e-01, -2.3698e-01,\n",
       "         -6.0167e-01,  5.3004e+00, -4.8213e-01, -3.1261e-01, -1.5710e-01,\n",
       "          3.4579e-01, -2.6160e-01,  1.4508e-01, -1.7959e-01,  7.9062e-02,\n",
       "          1.4472e-01,  3.2079e-01,  3.3514e-01, -1.1986e-02,  1.3734e-01,\n",
       "          2.3125e-01,  1.1890e-01, -7.3783e-01,  2.7982e-01, -4.4225e-02,\n",
       "          4.9915e-01,  3.3357e-01,  6.6946e-02, -2.7145e-01,  5.7290e-01,\n",
       "          3.5998e-01,  3.6748e-01, -4.7884e-02, -3.8459e-01, -1.2750e+00,\n",
       "         -7.5022e-02,  8.9474e-01, -1.2489e-01,  1.5591e-01, -1.5688e-01,\n",
       "          1.2372e-02, -9.4688e-02, -2.8592e-02, -2.8843e-01, -9.8846e-01,\n",
       "          2.1326e-01,  1.7536e-01, -2.5297e-01,  2.5644e-01,  2.4942e-03,\n",
       "         -3.2052e-01, -1.9876e-01, -2.0155e-01,  3.0926e-01,  2.8674e-01,\n",
       "         -3.0564e-01,  1.9946e-01, -4.8895e-01,  1.1833e-01, -2.1534e-01,\n",
       "         -2.9759e-03,  7.5457e-02, -2.7910e-01, -3.1856e-01,  1.9827e-01,\n",
       "          2.8155e-01, -3.6885e-01,  5.3372e-01, -3.2613e-01,  1.4964e-01,\n",
       "         -1.8078e-01, -4.2799e-01,  3.0540e-01,  2.8064e-01,  3.6734e-01,\n",
       "          9.4240e-03,  4.2914e-01,  3.9404e-01,  4.6019e-01, -6.4878e-01,\n",
       "         -1.2125e-01, -4.4394e-01, -1.4655e-02, -3.4848e-02, -1.4895e-01,\n",
       "         -9.7953e-02,  4.0241e-02, -3.7271e-01,  1.7165e-01,  1.8673e-01,\n",
       "         -5.8036e-01, -4.8151e-01,  3.5368e-01,  3.5170e-02,  3.0873e-01,\n",
       "         -2.3030e-02,  6.6635e-01, -1.3022e-01,  5.9723e-01,  4.0161e-01,\n",
       "         -6.2667e-01, -3.7437e-01, -8.7967e-02, -3.4177e-01,  2.1776e-01,\n",
       "          1.2268e-01,  4.0922e-01,  9.2390e-02, -8.1412e-01, -7.0540e-01,\n",
       "          1.7427e-01, -2.8874e-01,  1.4121e-02,  1.7298e-01, -1.2739e-01,\n",
       "          5.7629e-02, -3.1461e-01,  8.3595e-02, -1.1962e-01, -9.5062e-02,\n",
       "         -1.4173e-01, -7.3649e-02, -4.1208e-02, -3.7415e-01, -7.2083e-02,\n",
       "          3.5983e-01,  1.4615e-01,  2.0995e-01, -5.0964e-02,  2.2159e-01,\n",
       "         -2.2353e-03,  1.6769e-01, -3.2115e-01, -1.2446e-01, -7.3581e-02,\n",
       "          6.7402e-01,  7.1760e-02,  3.6850e-01,  1.7501e-01,  1.2283e-01,\n",
       "          2.1816e-01, -8.1428e-02,  5.2751e-02, -4.7673e-02, -1.6290e-02,\n",
       "         -2.6806e-01, -2.2986e-03,  1.7697e-01, -2.4244e-03,  3.1678e-01,\n",
       "          4.8429e-01, -9.0533e-02,  1.4315e-01,  3.4087e-01, -1.2568e-01,\n",
       "         -2.4048e-01, -5.5356e-01, -2.4369e-01, -6.5819e-01, -2.6690e-01,\n",
       "         -1.8780e-02, -5.7582e-02,  5.0926e-01, -1.7529e-01, -2.3294e-01,\n",
       "         -3.7647e-01, -1.9445e-01,  2.4455e-02,  7.6944e-01,  5.5507e-01,\n",
       "         -1.1932e-02,  4.1802e-02,  4.1322e-01, -8.8488e-01,  6.1653e-01,\n",
       "          6.1397e-02, -1.7352e-01,  4.0362e-02, -4.0826e-01,  4.9474e-01,\n",
       "         -2.4824e-02,  2.0981e-01, -1.2617e-01, -1.2043e-01, -9.5632e-02,\n",
       "          3.7488e-01,  4.5585e-01,  1.8535e-01, -1.5448e-01, -1.3467e+00,\n",
       "          1.9353e-01,  2.6100e-01,  2.9165e-03, -4.3129e-01, -3.1469e-01,\n",
       "          1.2961e-02,  2.2391e-02, -2.5527e-01,  4.5145e-01,  2.8739e-01,\n",
       "          2.9420e-01, -1.3818e-01, -2.0481e-01, -7.9752e-01,  2.5311e-02,\n",
       "         -3.4603e-01, -5.7642e-02,  1.3035e-01, -3.9415e-01, -1.2332e-01,\n",
       "         -2.2871e-03, -3.3633e-01,  6.2798e-01,  4.6228e-01, -8.3517e-02,\n",
       "         -3.4142e-02, -1.3833e-01,  2.8848e-02, -8.9790e-02,  1.3789e-01,\n",
       "         -2.7726e-03,  3.1105e-02,  3.0807e-01, -3.8679e-01,  4.1819e-01,\n",
       "         -1.0199e-02,  2.1021e-01,  1.6028e-01, -3.2025e+00, -4.6754e-01,\n",
       "          9.9293e-02,  5.6124e-01, -2.9790e-01,  3.7732e-01,  5.6371e-04,\n",
       "         -6.8516e-01,  2.1210e-01,  4.1273e-01,  7.1101e-03, -1.7327e-01,\n",
       "         -6.7691e-02,  2.9452e-01, -1.1069e-01,  9.8367e-01,  5.2116e-01,\n",
       "         -1.4262e-01, -2.0174e-01,  3.4074e-01,  4.1111e-01,  1.3994e-01,\n",
       "          1.5529e-01,  2.7999e-01, -9.4124e-01,  8.9809e-02, -4.7327e-01,\n",
       "         -3.3264e-01, -9.1099e-02, -2.6057e-01, -2.4796e-01, -1.5706e-01,\n",
       "         -4.1769e-01, -3.3268e-01, -3.2304e-01, -3.0439e-01, -1.3837e-01,\n",
       "         -3.4923e-01, -1.5625e-01, -8.4679e-01, -2.5944e-01, -3.3462e-01,\n",
       "          5.3804e-01, -1.3405e-01, -5.7516e-02, -3.4083e-01,  4.9636e-01,\n",
       "         -9.5073e-01,  6.6251e-01,  2.8586e-01, -2.0885e-01,  3.2850e-01,\n",
       "         -4.8590e-01, -7.0506e-01,  2.5873e-01,  2.4572e-01, -3.3798e-01,\n",
       "         -2.1587e-01, -1.3650e+00, -3.4320e-01,  1.7255e-01, -5.5296e-02,\n",
       "         -5.1691e-01,  9.9354e-02,  5.2803e-01,  1.6116e-01,  6.4038e-01,\n",
       "         -2.3602e-01, -1.6848e-01,  4.5264e-01,  9.4432e-02, -1.2238e+00,\n",
       "         -1.8977e-01, -2.4376e-02,  2.6735e-02,  6.6222e-02,  9.2119e-02,\n",
       "          3.0803e-02,  3.5836e-01, -2.6059e-01,  4.1740e-01,  2.9857e-02,\n",
       "          2.7132e-01,  4.0512e-01, -1.8235e-01, -4.1860e-01,  4.7378e-01,\n",
       "         -7.0791e-02, -1.0236e-01, -4.9181e-01,  4.7517e-01, -4.7181e-01,\n",
       "         -2.1566e-01,  4.8303e-02,  4.5641e-02, -2.5947e-01,  1.0815e-01,\n",
       "         -4.6282e-01,  4.8281e-01,  2.9094e-01,  1.0711e-01, -4.0455e-01,\n",
       "          4.3149e-01, -4.4203e-01,  2.5885e-01, -9.5563e-02, -5.0910e-01,\n",
       "         -2.3584e-01, -2.8591e-01, -1.7120e+00,  3.0152e-02, -2.6506e-01,\n",
       "         -1.0462e-01,  4.2638e-01, -3.2986e-01,  9.8991e-03,  1.5076e-01,\n",
       "         -9.0511e-01, -3.9121e-01, -4.3530e-01,  2.5190e-01, -3.0806e-01,\n",
       "         -1.2212e-01, -1.2090e+00, -3.9144e-01,  4.9997e-01,  5.0966e-02,\n",
       "          1.7531e+00,  5.9217e-01, -1.4565e-01, -3.7590e-01, -1.7275e-01,\n",
       "         -4.0564e-01,  4.1132e-01, -3.0553e-01,  1.1823e-01, -2.2291e-01,\n",
       "          1.1927e-01, -2.2481e-02, -3.2854e-01, -6.6563e-02, -1.7950e-01,\n",
       "          9.7420e-02, -1.1725e+00,  6.8930e-02, -4.3853e-01, -1.8034e-01,\n",
       "         -6.2387e-01, -3.2162e-01,  8.8298e-03,  7.9827e-01,  1.7623e-01,\n",
       "          4.3881e-01, -4.1666e-01, -3.1767e-01,  1.4750e-01, -1.3440e-01,\n",
       "          2.6665e-01,  1.9797e-01, -2.8327e-02,  1.4680e-01,  5.4564e-01,\n",
       "          4.7130e-02, -1.4113e-01, -3.8919e-01, -4.4484e-01, -1.1668e-01,\n",
       "         -5.0374e-01,  1.2378e-01, -6.2404e-01,  2.6293e-01, -1.2563e-01,\n",
       "          2.7573e-01,  4.3067e-02, -4.1996e-01,  9.4121e-02, -1.9801e-01,\n",
       "          3.3549e-01, -1.5326e-01,  1.7629e-01, -5.0505e-01,  1.8946e-01,\n",
       "         -2.6391e-01, -2.4149e-01,  4.0485e-01, -2.8218e-01,  9.7396e-01,\n",
       "          2.2447e-01,  1.6443e-01, -6.9449e-01, -1.3537e+00,  6.9135e-02,\n",
       "         -1.2476e+00,  3.6590e-01,  6.8629e-02, -3.3651e-02,  2.1899e-01,\n",
       "          1.1105e-01,  4.8752e-01, -4.0155e-01,  4.0849e-01,  2.3711e-01,\n",
       "          2.2353e-01, -3.3381e-01,  4.7354e-01,  9.4232e-01, -7.0992e-02,\n",
       "          3.3516e-01, -1.6412e-01,  3.6475e-01, -4.8580e-01,  1.0854e-01,\n",
       "          1.8187e-01,  4.2715e-01,  9.0932e-02,  3.6479e-01,  4.4273e-01,\n",
       "         -3.2144e-01, -4.1451e-02,  2.4553e-01, -1.7849e-01,  4.4097e-02,\n",
       "          2.9375e-01,  6.0538e-02, -1.6794e+00, -4.2336e-01, -1.2986e-01,\n",
       "          4.2880e-01, -3.0284e-01,  1.2192e-03, -1.1708e-01, -2.2456e-01,\n",
       "         -5.6587e-01,  1.1108e-01, -2.8181e-01, -2.1847e-01,  8.3335e+00,\n",
       "          2.0262e-01, -4.6229e-02, -3.6694e-01, -7.4657e-01, -1.0692e-01,\n",
       "          6.6177e-03,  7.8920e-03,  2.1925e-01]], grad_fn=<SelectBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import SiglipModel, AutoProcessor\n",
    "\n",
    "# siglip_model = SiglipModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "siglip_model = SiglipModel.from_pretrained(\"nielsr/siglip-base-patch16-224\")\n",
    "\n",
    "import torch\n",
    "x = torch.ones(1,3,224,224)\n",
    "\n",
    "siglip_model.vision_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(32000, 768)\n",
       "      (position_embedding): Embedding(64, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(196, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# x = torch.zeros(1,3,224,224)\n",
    "# siglip_model.vision_model(x).last_hidden_state.shape\n",
    "\n",
    "siglip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip import clip\n",
    "import torch\n",
    "model_path = clip._download(clip._MODELS['ViT-B/16'])\n",
    "\n",
    "try:\n",
    "    # loading JIT archive\n",
    "    model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "    state_dict = None\n",
    "\n",
    "except RuntimeError:\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    \n",
    "model = clip.build_model(state_dict or model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipTokenizer\n",
    "\n",
    "tokenizer = SiglipTokenizer.from_pretrained('google/siglip-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(['Hello There'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(['Hello There', 'hello there 2'], return_tensors='pt', padding='max_length')['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flsetup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
